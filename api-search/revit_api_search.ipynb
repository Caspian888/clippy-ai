{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LNIsiFvuAuv"
      },
      "source": [
        "# Building a Chatbot\n",
        "## Part I: Search and Retrieval\n",
        "\n",
        "### CORE Studio, Thornton Tomasetti\n",
        "\n",
        "#### Instructor: [Seyedomid Sajedi](https://www.linkedin.com/in/seyedomid-sajedi-263b703a)\n",
        "\n",
        "In this exercise, we will explore Retrieval Augmented Generation (RAG) and delve into prompt engineering for large language models. Our objective is to understand how to use these models effectively for Question-Answering (QA) tasks on custom datasets.\n",
        "\n",
        "This notebook represents the first step in a three-part process of creating an interactive chatbot:\n",
        "\n",
        "* Part I: Search and Retrieval\n",
        "* Part II: Prompt Engineering\n",
        "* Part III: Building a Chatbot UI\n",
        "\n",
        "In Parts I and II, we will break down the key components of our custom bot. In Part III, we will integrate these components into a chatbot with a user-friendly interface. Let's get started!\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhX4vZ7dwx7r"
      },
      "source": [
        "#Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_PvbehxB52gY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install rank_bm25 pypdf2 tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m9OykoRj6D6I"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade spacy # might be needed if the default spacy in colab is not working\n",
        "import spacy\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PyPDF2 import PdfReader\n",
        "from tqdm import tqdm\n",
        "import tiktoken\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gukXLv6vw4CZ"
      },
      "source": [
        "## spaCy\n",
        "[spaCy](https://spacy.io/) is a popular and powerful natural language processing (NLP) library that excels in tasks like tokenization, part-of-speech tagging, named entity recognition. Its efficiency and pre-trained models make it a great choice for developers, researchers, and businesses looking to work with text data, enabling them to quickly and accurately analyze and extract insights from large amounts of text, with support for multiple languages and a user-friendly API.\n",
        "\n",
        "We will use spaCy's tokenization for our exercise but it also comes with other useful features as well that we will quickly take a look at.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_wNwZj3zAKt"
      },
      "source": [
        "It is a good idea to remove english stop words from our tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-1uH_6HU8aal"
      },
      "outputs": [],
      "source": [
        "nlp_model = spacy.load(\"en_core_web_sm\")\n",
        "# Get the list of stop words\n",
        "stop_words = list(nlp_model.Defaults.stop_words)\n",
        "# print(len(stop_words))\n",
        "# print(stop_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_An6KjDm0Dgh"
      },
      "source": [
        "We will use spaCy to tokenize both reference documents in our database and user search queries. The following function handles tokenization for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jJFk7ylwBdbq"
      },
      "outputs": [],
      "source": [
        "def spacy_tokenizer(text, nlp):\n",
        "    doc = nlp(text)\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        # Check if the token is not punctuation and not a stop word\n",
        "        if not (token.is_punct or token.is_stop):\n",
        "            tokens.append(token.lemma_.lower())\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "35xw11Cz8lPd"
      },
      "outputs": [],
      "source": [
        "openai_tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mR3-J_q00YS"
      },
      "source": [
        "The next function will grab a pdf document from a url and loads it into memory. You can alternatively upload a document to Colab or your working directory and load it manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JDRIwd6VH11A"
      },
      "outputs": [],
      "source": [
        "# We used this function for topic modeling\n",
        "def get_pdf_as_memory_stream(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an error for HTTP errors\n",
        "    # Convert the response content into a BytesIO stream\n",
        "    return BytesIO(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zYheyVw194ru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36ea2fce-a243-4a42-850f-cff669809858"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "directory_path = '/content/drive/MyDrive/Learn/Coding/Revit APIs/2021.1/html/'\n",
        "\n",
        "# Get all files in directory\n",
        "files_in_directory = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))] # entire API\n",
        "# files_in_directory = [f for f in os.listdir(directory_path)[0:5] if os.path.isfile(os.path.join(directory_path, f))] # subset for testing\n",
        "\n",
        "print(len(files_in_directory))"
      ],
      "metadata": {
        "id": "FKwj83Of-bEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_docs = []\n",
        "for file_name in files_in_directory:\n",
        "    # print(file_name)\n",
        "    full_path = os.path.join(directory_path, file_name)  # Get full path to file\n",
        "    with open(full_path, 'r', encoding='utf-8') as file:  # Add encoding='utf-8' to handle any special characters in the HTML files\n",
        "        soup = BeautifulSoup(file, 'html.parser')\n",
        "        # print(soup.title.string if soup.title else \"No title\")\n",
        "        # print(soup)\n",
        "\n",
        "        # syntax_section = soup.find('div', id='syntaxSection')\n",
        "        # remarks_section = soup.find('div', id='remarksSection')\n",
        "        # syntax_code_blocks = soup.find('div', id='syntaxCodeBlocks')\n",
        "        # nsr_title = soup.find('span', id='nsrTitle')\n",
        "\n",
        "        # output = [syntax_section, remarks_section, syntax_code_blocks, nsr_title]\n",
        "        # print(output)\n",
        "\n",
        "        csharp_code = soup.find('span', codelanguage='CSharp')\n",
        "\n",
        "        if csharp_code:\n",
        "          code = csharp_code.get_text()[2:]\n",
        "        else:\n",
        "          code = \"\"\n",
        "\n",
        "        print(code)\n",
        "        print(\"-\"*100)\n",
        "\n",
        "        api_docs.append(f\"{code} \")\n",
        "        # api_docs.append(soup)\n",
        "        # api_docs.append(str(soup.prettify()))\n"
      ],
      "metadata": {
        "id": "R3kbglh-qOP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# should move condition before append to be more efficient\n",
        "# don't append if item == ' '\n",
        "\n",
        "while(' ' in api_docs):\n",
        "  api_docs.remove(' ')\n",
        "\n",
        "print(api_docs)"
      ],
      "metadata": {
        "id": "HiaMJFx7NXOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP2S0Mcn2uRJ"
      },
      "source": [
        "## Indexing the search database\n",
        "Large language models have a limit cap on the amount of text they can handle for each conversation. We have to keep that in mind when building our search databse. It is ideal to chunk your long documents into small and independent sections. This smart chunking, however, could be very challenging to automate for large datasets. It is also important to keep the chunks roughly the same size. Having very long and very short chunks could adversly affect the quality of search.\n",
        "\n",
        "For this exercise, we will simply split the document page by page. In a real project, it is often worth spending on a more delibrate strategy to index and chunk your text dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2yfGq42Q_5H"
      },
      "outputs": [],
      "source": [
        "# tokenize dataset\n",
        "tokenized_dataset= []\n",
        "text_dataset =[]\n",
        "\n",
        "# from beautiful-soup\n",
        "print(tqdm)\n",
        "print(len(api_docs))\n",
        "for html in api_docs:\n",
        "    # print(html)\n",
        "    html = str(html)\n",
        "    text_dataset.append(html)\n",
        "    tokenized_text = spacy_tokenizer(html, nlp_model)\n",
        "    print(tokenized_text)\n",
        "    tokenized_dataset.append(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iKryIOIRsR8"
      },
      "outputs": [],
      "source": [
        "print(len(tokenized_dataset))\n",
        "print(tokenized_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ-Ev3Th4Lrb"
      },
      "source": [
        "## Search Algorithm\n",
        "Search algorithms play a pivotal role in various fields. Their complexity stems from the need to efficiently navigate vast datasets or solution spaces, making them indispensable tools for finding relevant information and optimizing decision-making processes in the modern world. For our search engine, we will utilize a classical and popular ranking model called [BM25](https://en.wikipedia.org/wiki/Okapi_BM25).\n",
        "\n",
        "**Note:**\n",
        "\n",
        "BM25 is a keyword sensitive (lexical) search algorithm. It calculates a relavance score between a given user query and reference documents. Does this sound familiar to anything we did befin the previous exercises?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLweGdDjNpqN"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "def init_bm25_vectorizer():\n",
        "    return BM25Okapi(tokenized_dataset)\n",
        "bm25_vectorizer = init_bm25_vectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhMivpx1SSCn"
      },
      "outputs": [],
      "source": [
        "# search\n",
        "query=\"What code do i need to create a sheet in revit?\"\n",
        "query_tokens = spacy_tokenizer(query,nlp_model)\n",
        "query_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICTqNUNqSxhx"
      },
      "outputs": [],
      "source": [
        "bm25_scores = bm25_vectorizer.get_scores(query_tokens)\n",
        "print(bm25_scores.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax_oceEV8SMh"
      },
      "source": [
        "After scoring the documents based on relevance, they can be sorted to get the top k results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB7U4oNjS8Ko"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "top_k = 3 # Number of top k pages based on BM25 score\n",
        "top_page_indx= np.argsort(bm25_scores)[-top_k:][::-1]\n",
        "hits = [{'page_indx': idx, 'score': bm25_scores[idx]} for idx in top_page_indx]\n",
        "hits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wENpOWbv8xtG"
      },
      "source": [
        "Now, let's print the retrieved documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RB3kEyDRcZD8"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "total_tokens = []\n",
        "reference_list = []\n",
        "\n",
        "for hit in hits:\n",
        "  page_str = text_dataset[hit['page_indx']]\n",
        "  openai_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "  token_count = len(openai_tokenizer.encode(page_str))\n",
        "  total_tokens.append(token_count)\n",
        "  reference_list.append(page_str)\n",
        "\n",
        "  # print(f\"PAGE: {hit['page_indx']+1}, token count:{token_count}\")\n",
        "  # print(\"\\n\")\n",
        "  # pprint.pprint(f\"PAGE: {hit['page_indx']+1}, token count:{token_count}\")\n",
        "  # print(\"\\n\")\n",
        "\n",
        "\n",
        "  # print(page_str)\n",
        "  # print(\"-\"*50)\n",
        "\n",
        "all_references = ('\\n\\n'.join(reference_list))\n",
        "print(all_references)\n",
        "print(sum(total_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P33tDnP99BIP"
      },
      "source": [
        "Finally we can create a clean function for our document retrieval:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgYSBUHD9Ilr"
      },
      "outputs": [],
      "source": [
        "# doc_retrieval('Move door at Gridline J/2 3 ft to the right', 10)\n",
        "# doc_retrieval('collector = FilteredElementCollector(doc).OfCategory(BuiltInCategory.OST_StructuralColumns).WhereElementIsNotElementType()', 10)\n",
        "# doc_retrieval('filtered element collector', 3)\n",
        "# doc_retrieval('collector', 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPHUiNYDmPMo"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "def doc_retrieval(query,\n",
        "                  top_k=3):\n",
        "  #  we need to load bm25_vectorizer,nlp_model in other scripts\n",
        "  query_tokens = spacy_tokenizer(query,nlp_model)\n",
        "  bm25_scores = bm25_vectorizer.get_scores(query_tokens)\n",
        "  top_page_indx= np.argsort(bm25_scores)[-top_k:][::-1]\n",
        "  hits = [{'page_indx': idx, 'score': bm25_scores[idx]} for idx in top_page_indx]\n",
        "\n",
        "  total_tokens = []\n",
        "  reference_list = []\n",
        "\n",
        "  print('-'*100)\n",
        "  print(f\"Query: {query}\")\n",
        "  print('-'*100)\n",
        "\n",
        "  for hit in hits:\n",
        "    page_str = text_dataset[hit['page_indx']]\n",
        "    openai_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    token_count = len(openai_tokenizer.encode(page_str))\n",
        "    total_tokens.append(token_count)\n",
        "    reference_list.append(page_str)\n",
        "    pprint.pprint(page_str)\n",
        "\n",
        "  print(f\"\\nToken count from reference text: {sum(total_tokens)}\")\n",
        "  print('-'*100, '\\n')\n",
        "\n",
        "  all_references = ('\\n\\n'.join(reference_list))\n",
        "  return all_references"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1z6pUrAnB2-"
      },
      "outputs": [],
      "source": [
        "doc_retrieval('delete all elements', 50)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}